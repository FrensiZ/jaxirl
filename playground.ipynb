{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playground to understand the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etils.epath found. Using etils.epath for file I/O.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from gymnax.environments import environment, spaces\n",
    "from typing import Tuple, Optional\n",
    "import chex\n",
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_obs(num_columns, num_rows, num_rewards, obs):\n",
    "    # Reshape the observation to match the grid dimensions\n",
    "    obs = obs.reshape(num_columns, num_rows, -1)\n",
    "    # Extract agents position and rewards\n",
    "    state = jnp.asarray(jnp.nonzero(obs[:, :, 1], size=(1), fill_value=num_columns))\n",
    "    \n",
    "    for num_reward in range(2, num_rewards + 2):\n",
    "        reward = jnp.asarray(\n",
    "            jnp.nonzero(obs[:, :, num_reward], size=(1), fill_value=num_columns)\n",
    "        )\n",
    "        state = jnp.concatenate([state, reward], axis=0)\n",
    "    \n",
    "    # The state is then represented as a concatenated array of positions (agent and rewards) and returned as the environment's state.\n",
    "    return state.reshape(-1)\n",
    "\n",
    "\n",
    "def get_new_found_array(matching_pos, old_found_array):\n",
    "    # Checks whether a specific position (likely of a reward) has been found and updates the corresponding boolean array\n",
    "    index = jax.numpy.nonzero(matching_pos, size=1)[0][0]\n",
    "    return old_found_array.at[index].set(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a maze with rooms, creating vertical and horizontal walls with doors randomly placed.\n",
    "# The maze is represented as a grid of ones (open spaces) and zeros (walls), with specific cells set to 1 to create doors in the walls.\n",
    "def rooms_maze(\n",
    "    key,\n",
    "    columns: int,\n",
    "    rows: int,\n",
    "    vertical_wall_y: Optional[int] = 1,\n",
    "    horizontal_wall_x: Optional[int] = 1,\n",
    "):\n",
    "    grid = jnp.ones((rows, columns))\n",
    "    rng1, rng2, rng3, rng4 = jax.random.split(key, 4)\n",
    "\n",
    "    # max value is exclusive\n",
    "    vertical_wall_door_up = jax.random.randint(\n",
    "        next(rng1), shape=(), minval=0, maxval=horizontal_wall_x\n",
    "    )\n",
    "    vertical_wall_door_down = jax.random.randint(\n",
    "        next(rng2), shape=(), minval=horizontal_wall_x + 1, maxval=rows\n",
    "    )\n",
    "    horizontal_wall_door_left = jax.random.randint(\n",
    "        next(rng3), shape=(), minval=0, maxval=vertical_wall_y\n",
    "    )\n",
    "    horizontal_wall_door_right = jax.random.randint(\n",
    "        next(rng4), shape=(), minval=vertical_wall_y + 1, maxval=columns\n",
    "    )\n",
    "    vertical_wall_door_up = 0\n",
    "    vertical_wall_door_down = 5\n",
    "    horizontal_wall_door_left = 2\n",
    "    horizontal_wall_door_right = 4\n",
    "\n",
    "    grid = grid.at[horizontal_wall_x, :].set(0.0)\n",
    "    grid = grid.at[:, vertical_wall_y].set(0.0)\n",
    "    grid = grid.at[\n",
    "        horizontal_wall_x, [horizontal_wall_door_left, horizontal_wall_door_right]\n",
    "    ].set(1.0)\n",
    "    grid = grid.at[\n",
    "        [vertical_wall_door_up, vertical_wall_door_down], vertical_wall_y\n",
    "    ].set(1.0)\n",
    "    return jnp.array(grid, dtype=jnp.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wall_maze(\n",
    "    columns: int,\n",
    "    rows: int,\n",
    "    vertical: bool = True,\n",
    "    wall: Optional[int] = 1,\n",
    "    door: Optional[int] = 1,\n",
    "):\n",
    "    grid = jnp.ones((rows, columns))\n",
    "\n",
    "    vertical_grid = grid.at[wall, :].set(0.0)\n",
    "    horizontal_grid = grid.at[:, wall].set(0.0)\n",
    "    vertical_grid = vertical_grid.at[wall, door].set(1.0)\n",
    "    horizontal_grid = horizontal_grid.at[door, wall].set(1.0)\n",
    "    grid = jax.lax.select(vertical, vertical_grid, horizontal_grid)\n",
    "    return jnp.array(grid, dtype=jnp.bool_)\n",
    "\n",
    "\n",
    "def generate_walls(\n",
    "    key,\n",
    "    rows: int,\n",
    "    columns: int,\n",
    "    wall: bool = True,\n",
    "    wall_x: int = None,\n",
    "    wall_y: int = None,\n",
    "):\n",
    "    return jax.lax.select(\n",
    "        wall,\n",
    "        rooms_maze(key, columns, rows, wall_x, wall_y),\n",
    "        jnp.ones((rows, columns), dtype=jnp.bool_),\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_one_wall(\n",
    "    rows: int,\n",
    "    columns: int,\n",
    "    wall: bool = True,\n",
    "    vertical: bool = True,\n",
    "    wall_pos: int = None,\n",
    "    door_pos: int = None,\n",
    "):\n",
    "    return jax.lax.select(\n",
    "        wall,\n",
    "        wall_maze(columns, rows, vertical, wall_pos, door_pos),\n",
    "        jnp.ones((rows, columns), dtype=jnp.bool_),\n",
    "    )\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class Observations:\n",
    "    shape: chex.Array\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class EnvState:\n",
    "    pos: chex.Array\n",
    "    goals: chex.Array\n",
    "    found: chex.Array\n",
    "    right_order: bool\n",
    "    grid_env: chex.Array\n",
    "    time: int\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class EnvParams:\n",
    "    maze: chex.Array\n",
    "\n",
    "\n",
    "#   wall: int = 1\n",
    "#   wall_x: int = 1\n",
    "#   wall_y: int = 1\n",
    "\n",
    "# jax.tree_util.register_pytree_node(EnvParams, EnvParams.tree_flatten, EnvParams.tree_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldNew(environment.Environment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rows=5,\n",
    "        columns=5,\n",
    "        num_rewards=2,\n",
    "        max_steps_in_episode=30,\n",
    "        last_reward_stays=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        self.grid_indexes = (\n",
    "            jnp.indices([rows, columns]).transpose(1, 2, 0).reshape(-1, 2)\n",
    "        )\n",
    "        self.num_rewards = num_rewards\n",
    "        self.max_steps_in_episode = max_steps_in_episode\n",
    "        self.directions = jnp.array([[-1, 0], [0, 1], [1, 0], [0, -1]])\n",
    "        self.last_reward_stays = last_reward_stays\n",
    "        reward = jnp.arange(self.num_rewards)\n",
    "        self.reward_order = jnp.argsort(reward)\n",
    "\n",
    "    @property\n",
    "    def default_params(self) -> EnvParams:\n",
    "        # default env parameters\n",
    "        return EnvParams(jnp.ones((self.columns, self.rows), dtype=jnp.bool_))\n",
    "\n",
    "    # @partial(jax.jit, static_argnames=['params'])\n",
    "    # @jax.jit\n",
    "    def step_env(\n",
    "        self, key: chex.PRNGKey, state: EnvState, action: int, params: EnvParams\n",
    "    ) -> Tuple[chex.Array, EnvState, float, bool, dict]:\n",
    "        \"\"\"Performs step transitions in the environment.\n",
    "\n",
    "        Returns: env_state, obsv, reward, done, info\n",
    "        \"\"\"\n",
    "        # Move the agent\n",
    "        p = state.pos + self.directions[action]\n",
    "        px = jnp.clip(p[0], 0, self.rows - 1)\n",
    "        py = jnp.clip(p[1], 0, self.columns - 1)\n",
    "        in_map = state.grid_env[px, py]\n",
    "        new_pos = jax.lax.select(in_map, jnp.array([px, py]), state.pos)\n",
    "\n",
    "        # e.g. [[False, False], [True, True], [True, False]]\n",
    "        pos_match = jnp.equal(state.goals, jnp.tile(p, [self.num_rewards, 1]))\n",
    "        # e.g. [False, True, False]\n",
    "        found_goal_arr = jax.numpy.all(pos_match, axis=1)\n",
    "        # e.g. True\n",
    "        found_goal = jnp.any(found_goal_arr)\n",
    "        # e.g. 1\n",
    "        found_goal_idx = jnp.nonzero(found_goal_arr, size=1)[0][0]\n",
    "        found_before = jax.lax.select(\n",
    "            found_goal,\n",
    "            jnp.bool_(state.found[found_goal_idx]),\n",
    "            jnp.array(True, dtype=jnp.bool_),\n",
    "        )\n",
    "        total_goals_found = jnp.uint8(state.found).sum()\n",
    "        goal_reached = jnp.logical_and(found_goal, jnp.logical_not(found_before))\n",
    "        goal_correct_order = jnp.logical_and(\n",
    "            goal_reached, self.reward_order[found_goal_idx] == total_goals_found\n",
    "        )\n",
    "\n",
    "        # Update state dict and evaluate termination conditions\n",
    "        new_found = jax.lax.select(\n",
    "            goal_correct_order,\n",
    "            get_new_found_array(found_goal_arr, state.found),\n",
    "            state.found,\n",
    "        )\n",
    "        new_found = jax.lax.select(\n",
    "            jnp.logical_and(\n",
    "                total_goals_found == self.num_rewards - 1, self.last_reward_stays\n",
    "            ),\n",
    "            state.found,\n",
    "            new_found,\n",
    "        )\n",
    "        new_state = EnvState(\n",
    "            new_pos,\n",
    "            state.goals,\n",
    "            new_found,\n",
    "            state.right_order,\n",
    "            state.grid_env,\n",
    "            state.time + 1,\n",
    "        )\n",
    "        done = self.is_terminal(state, params)\n",
    "        obs = lax.stop_gradient(self.get_obs(new_state))\n",
    "        return (\n",
    "            obs,\n",
    "            lax.stop_gradient(new_state),\n",
    "            goal_correct_order.astype(jnp.float32),\n",
    "            done,\n",
    "            {},\n",
    "        )\n",
    "\n",
    "    # @partial(jax.jit, static_argnames=['params'])\n",
    "    # @jax.jit\n",
    "    def reset_env(\n",
    "        self, key: chex.PRNGKey, params: EnvParams\n",
    "    ) -> Tuple[chex.Array, EnvState]:\n",
    "        \"\"\"Performs resetting of environment.\n",
    "\n",
    "        Returns: state, obs\n",
    "        \"\"\"\n",
    "        k1, k2 = jax.random.split(key)\n",
    "        # grid_env = generate_walls(k1, self.rows, self.columns, params.wall, params.wall_x, params.wall_y)\n",
    "        grid_env = params.maze\n",
    "        goal_pos, agent_pos = sample_init_state(\n",
    "            k2, grid_env, self.grid_indexes, self.num_rewards, params\n",
    "        )\n",
    "        rewards_found = jnp.array([False] * self.num_rewards)\n",
    "        state = EnvState(agent_pos, goal_pos, rewards_found, True, grid_env, 0)\n",
    "        # return self.get_obs(state), state\n",
    "        return self.get_obs(state), state\n",
    "\n",
    "    def get_obs(self, state: EnvState) -> chex.Array:\n",
    "        \"\"\"Applies observation function to state.\"\"\"\n",
    "        # wall = 0\n",
    "        # agent = 1\n",
    "        # rewards >= 2\n",
    "        one_hot_enc_obs = jnp.zeros((self.columns, self.rows, self.num_rewards + 2))\n",
    "        one_hot_enc_obs = one_hot_enc_obs.at[:, :, 0].set((1 - state.grid_env))\n",
    "        one_hot_enc_obs = one_hot_enc_obs.at[\n",
    "            state.goals[:, 0], state.goals[:, 1], jnp.arange(2, self.num_rewards + 2)\n",
    "        ].set(jnp.logical_not(state.found).astype(int))\n",
    "        one_hot_enc_obs = one_hot_enc_obs.at[state.pos[0], state.pos[1], 1].set(1.0)\n",
    "        return one_hot_enc_obs.flatten()\n",
    "\n",
    "    def get_obs_ints(self, state: EnvState) -> chex.Array:\n",
    "        return jnp.concatenate(\n",
    "            (state.pos, state.goals.flatten(), state.found.astype(int)), axis=-1\n",
    "        )\n",
    "\n",
    "    def is_terminal(self, state: EnvState, params: EnvParams) -> bool:\n",
    "        \"\"\"Check whether state is terminal.\"\"\"\n",
    "        # Check termination criteria\n",
    "        done = jnp.all(state.found)\n",
    "\n",
    "        # Check number of steps in episode termination condition\n",
    "        done_steps = state.time >= self.max_steps_in_episode - 1\n",
    "        done = jnp.logical_or(done, done_steps)\n",
    "        return done\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Environment name.\"\"\"\n",
    "        return \"GridWorld\"\n",
    "\n",
    "    @property\n",
    "    def num_actions(self) -> int:\n",
    "        \"\"\"Number of actions possible in environment.\"\"\"\n",
    "        return 4\n",
    "\n",
    "    def observation_space(self, params: EnvParams) -> spaces.Box:\n",
    "        \"\"\"Observation space of the environment.\"\"\"\n",
    "        return spaces.Box(\n",
    "            0,\n",
    "            1,\n",
    "            (self.columns * self.rows * (self.num_rewards + 2),),\n",
    "            dtype=jnp.float32,\n",
    "        )\n",
    "\n",
    "    def observation_space_reward(self, params: EnvParams) -> spaces.Box:\n",
    "        \"\"\"Observation space of the environment.\"\"\"\n",
    "        return spaces.Box(\n",
    "            0, 1, (self.columns * self.rows * (self.num_rewards + 2)), dtype=jnp.float32\n",
    "        )\n",
    "\n",
    "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Discrete:\n",
    "        \"\"\"Action space of the environment.\"\"\"\n",
    "        return spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_init_state(\n",
    "    key: chex.PRNGKey,\n",
    "    grid_env: chex.Array,\n",
    "    grid_indexes: chex.Array,\n",
    "    num_rewards: int,\n",
    "    params: EnvParams,\n",
    ") -> Tuple[chex.Array, chex.Array]:\n",
    "    \"\"\"Sample a new initial state.\"\"\"\n",
    "    rng = hk.PRNGSequence(key)\n",
    "    pos_indexes = get_random_position(\n",
    "        grid_env, grid_indexes, num_rewards + 1, next(rng)\n",
    "    )\n",
    "    # pos_index = jnp.array([2,0])\n",
    "    # goal_indexes = get_random_position(grid_env, grid_indexes, num_rewards, next(rng))\n",
    "    # goal_indexes = jnp.array([[0,2]])\n",
    "    goal_indexes = pos_indexes[1:, :]\n",
    "    return goal_indexes, pos_indexes[0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
